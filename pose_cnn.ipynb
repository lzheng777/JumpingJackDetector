{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pose_cnn.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1YqHZqzPcTn1SI_V0yAC7GRDZYN7UF8sZ","authorship_tag":"ABX9TyMk+yuRzeMg+yDKN8br74Fm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"auf6y99V7d1W"},"source":["Code for training KeyPoint R-CNN on Coco Dataset and MPI Dataset (Not used in final project)"]},{"cell_type":"code","metadata":{"id":"MIz3NbS6xym8"},"source":["import os\n","import numpy as np\n","import torch\n","import torch.utils.data\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import torchvision\n","import os\n","from imutils import paths\n","import scipy.io\n","import pandas as pd\n","import csv\n","import skimage\n","from skimage import io, transform\n","from pycocotools.coco import COCO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QOoKNnibyJkk"},"source":["print(torch.cuda.device_count())\n","print('Device:', torch.device('cuda:0'))\n","\n","if torch.cuda.is_available():\n","  device = torch.device('cuda:0')\n","else:\n","  device = torch.device('cpu')\n","  \n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"glhcb-igdAo1"},"source":["!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n","!unzip annotations_trainval2017.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1X3GSCnc5EXr"},"source":["TRAIN_ANNO_PATH = '/content/annotations/person_keypoints_train2017.json'\n","VALID_ANNO_PATH = '/content/annotations/person_keypoints_val2017.json'\n","train_dataset = COCO(TRAIN_ANNO_PATH)\n","val_dataset = COCO(VALID_ANNO_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_g43db_6wGkT"},"source":["class CocoDataset(Dataset):\n","    # initialise function of class\n","    def __init__(self, root, annotation, transforms=None):\n","        # the data directory \n","        self.root = root\n","        self.transforms = transforms\n","        self.coco = COCO(annotation)\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","\n","    # obtain the sample with the given index\n","    def __getitem__(self, index):\n","        # Image ID\n","        img_id = self.ids[index]\n","        # List: get annotation id from coco\n","        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n","        # Dictionary: target coco_annotation file for an image\n","        coco_annotation = self.coco.loadAnns(ann_ids)\n","        # path for input image\n","        path = self.coco.loadImgs(img_id)[0]['file_name']\n","        # open the input image\n","        image = Image.open(os.path.join(self.root, path))\n","        \n","        # number of objects in the image\n","        num_objs = len(coco_annotation)\n","\n","        # Bounding boxes for objects\n","        # In coco format, bbox = [xmin, ymin, width, height]\n","        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n","        boxes = []\n","        for i in range(num_objs):\n","            xmin = coco_annotation[i]['bbox'][0]\n","            ymin = coco_annotation[i]['bbox'][1]\n","            xmax = xmin + coco_annotation[i]['bbox'][2]\n","            ymax = ymin + coco_annotation[i]['bbox'][3]\n","            boxes.append([xmin, ymin, xmax, ymax])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # Labels (In my case, I only one class: target class or background)\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        # Tensorise img_id\n","        img_id = torch.tensor([img_id])\n","        # Size of bbox (Rectangular)\n","        areas = []\n","        for i in range(num_objs):\n","            areas.append(coco_annotation[i]['area'])\n","        areas = torch.as_tensor(areas, dtype=torch.float32)\n","        # Iscrowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","        keypoints = []\n","        for i in range(num_objs):\n","            keypoints.append(coco_annotation[i]['keypoints'])\n","        keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n","\n","        # Annotation is in dictionary format\n","        targets = {}\n","        targets[\"boxes\"] = boxes\n","        targets[\"labels\"] = labels\n","        targets[\"image_id\"] = img_id\n","        targets[\"area\"] = areas\n","        targets[\"iscrowd\"] = iscrowd\n","        targets[\"keypoints\"] = keypoints\n","\n","        if self.transforms is not None:\n","            image = self.transforms(image)\n","\n","        return image, targets\n","    \n","    def __len__(self):\n","        return len(self.ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5wQhrof1gha"},"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n","\n","coco_dataset = CocoDataset('/content/drive/MyDrive/EE381K/train2017', \n","                           '/content/annotations/person_keypoints_train2017.json',\n","                           transforms=transforms)\n","data_loader = torch.utils.data.DataLoader(coco_dataset,\n","                                          batch_size=4,\n","                                          shuffle=True,\n","                                          num_workers=2,\n","                                          collate_fn=collate_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gCHsDC5n5nCs"},"source":["num_epochs = 3\n","model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False)\n","\n","# move model to the right device\n","model.to(device)\n","    \n","# parameters\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","\n","len_dataloader = len(data_loader)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    i = 0    \n","    for imgs, annotations in data_loader:\n","        i += 1\n","        imgs = list(img.to(device) for img in imgs)\n","        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n","        if annotations[0]['boxes'].size() == torch.Size([0]):\n","            continue\n","        loss_dict = model(imgs, annotations)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        if i % 2000 == 0:\n","            path = '/content/drive/MyDrive/ColabNotebooks/pose_rcnn_{}.pt'.format(str(i / 2000))\n","            torch.save(model.state_dict(), path)\n","            print('saved model')\n","\n","    # saving trained model\n","    path = '/content/drive/MyDrive/ColabNotebooks/pose_rcnn_{}.pt'.format(epoch)\n","    torch.save(model.state_dict(), path)\n","    print('saved model')\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCfXa27ldRcr","executionInfo":{"status":"ok","timestamp":1618639922905,"user_tz":300,"elapsed":649,"user":{"displayName":"Lucy W Zheng","photoUrl":"","userId":"04240352055010535427"}},"outputId":"8b8ef995-ab78-4d3b-cb48-cf178bbe4b1f"},"source":["  PATH = '/content/drive/MyDrive/ColabNotebooks/pose_rcnn.pt'\n","  torch.save(model.state_dict(), PATH)\n","  print('saved model')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["saved model\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nNBqXqZi5rB2"},"source":["Below uses the MPI Dataset to train the model"]},{"cell_type":"code","metadata":{"id":"ScUd3Ht-ztt0"},"source":["class PoseDataset(Dataset):\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        self.landmarks_frame = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return len(self.landmarks_frame)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.landmarks_frame.iloc[idx, 1])\n","\n","        image = io.imread(img_name)\n","        image = skimage.img_as_float(image)\n","        landmarks = self.landmarks_frame.iloc[idx, 2:34]\n","        landmarks = np.array([landmarks])\n","        landmarks = landmarks.astype('float').reshape(-1, 2)\n","\n","        sample = {'image': image, 'targets': landmarks}\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        boxes = self.find_box(sample['image'].size(), sample['targets'])\n","\n","        targets = {}\n","        targets['boxes'] = boxes\n","        targets[\"labels\"] = torch.tensor(1)\n","        targets[\"image_id\"] = torch.tensor(int(self.landmarks_frame.iloc[idx, 1].split('.')[0]))\n","        targets['keypoints'] = sample['targets']\n","\n","        return sample['image'], targets\n","\n","\n","    def map_keypose(self, landmarks):\n","        targets = {}\n","        targets['boxes'] = boxes\n","        targets['r_ankle'] = landmarks[0]\n","        targets['r_knee'] = landmarks[1]\n","        targets['r_hip'] = landmarks[2]\n","        targets['l_hip'] = landmarks[3]\n","        targets['l_knee'] = landmarks[4]\n","        targets['l_ankle'] = landmarks[5]\n","        targets['pelvis'] = landmarks[6] \n","        targets['thorax'] = landmarks[7]\n","        targets['upper_neck'] = landmarks[8]\n","        targets['head_top'] = landmarks[9]\n","        targets['r_wrist'] = landmarks[10]\n","        targets['r_elbow'] = landmarks[11]\n","        targets['r_shoulder'] = landmarks[12]\n","        targets['l_wrist'] = landmarks[13]\n","        targets['l_elbow'] = landmarks[14]\n","        targets['l_shoulder'] = landmarks[15]\n","        return targets\n","\n","    def find_box(self, image_size, landmarks):\n","        boxes = []\n","        xmin = 1000\n","        xmax = 0\n","        ymin = 1000\n","        ymax = 0\n","\n","        for coord in landmarks:\n","            if coord[0] < xmin:\n","                xmin = coord[0]\n","            if coord[0] > xmax:\n","                xmax = coord[0]\n","            if coord[1] < ymin:\n","                ymin = coord[1]\n","            if coord[1] > ymax:\n","                ymax = coord[1]\n","\n","        h, w = image_size[1], image_size[2]\n","        xmin = max(0, xmin)\n","        xmax = min(w, xmax)\n","        ymin = max(0, ymin)\n","        ymax = min(h, ymax)\n","\n","        boxes.append([xmin, ymin, xmax, ymax])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        boxes = torch.squeeze(boxes)\n","        return boxes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mwpwYAIPvs6U"},"source":["class RandomCrop(object):\n","    def __init__(self, output_size):\n","        self.output_size = (output_size, output_size)\n","\n","\n","    def __call__(self, sample):\n","        image, landmarks = sample['image'], sample['targets']\n","\n","        h, w = image.shape[:2]\n","        new_h, new_w = self.output_size\n","\n","        top = np.random.randint(0, h - new_h)\n","        left = np.random.randint(0, w - new_w)\n","\n","        image = image[top: top + new_h,\n","                      left: left + new_w]\n","\n","        landmarks = landmarks - [left, top]\n","\n","        return {'image': image, 'targets': landmarks}\n","\n","\n","class ToTensor(object):\n","    def __call__(self, sample):\n","        image, landmarks = sample['image'], sample['targets']\n","\n","        # swap color axis because\n","        # numpy image: H x W x C\n","        # torch image: C X H X W\n","        image = image.transpose((2, 0, 1))\n","        return {'image': torch.from_numpy(image).float(), 'targets': torch.from_numpy(landmarks).float()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RZ8KwwHY-R0"},"source":["IMAGE_DIR = '/content/drive/MyDrive/EE381K/PoseDS'\n","CSV_FILE = '/content/drive/MyDrive/Colab_Notebooks/mpii_dataset.csv'\n","\n","transformations = torchvision.transforms.Compose([ToTensor()])\n","pose_dataset = PoseDataset(CSV_FILE, IMAGE_DIR, transform=transformations)\n","data_loader = torch.utils.data.DataLoader(pose_dataset, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDA_wHjGZCyz"},"source":["# load an instance segmentation model pre-trained on COCO\n","model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VU6MCixBZjH8"},"source":["print('Starting training')\n","model.train()\n","\n","for epoch in range(3):  # loop over the dataset multiple times\n","    running_loss = 0.0\n","\n","    for i, data in enumerate(data_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        labels = [{t: labels[t].to(device) for t in labels}]        \n","        inputs = inputs.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model.forward(inputs, labels)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 1000 == 999:    # print every 1000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                (epoch + 1, i + 1, running_loss / 1000))\n","            running_loss = 0.0\n","            torch.save(model.state_dict(), '/content/chkp/check{}-{}.pt'.format(epoch, int((i + 1) / 1000)))\n","\n","print('Finished Training')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQKY47r4XbDl"},"source":["# saving trained model\n","PATH = r'/content/pose_cnn.pt'\n","torch.save(model.state_dict(), PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DS-t6shUdktV"},"source":["# testing the model\n","model.eval()\n","\n","temp_dataset = torchvision.datasets.ImageFolder('/content/demo')\n","temp_dataloader = DataLoader(temp_dataset)\n","\n","correct = 0\n","total = 1\n","with torch.no_grad():\n","    for data in data_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model.forward(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        break\n","\n","if total != 0:\n","    total = total - 1\n","print('correct: {}, total: {}'.format(correct, total))"],"execution_count":null,"outputs":[]}]}